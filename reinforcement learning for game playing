import random

# Define the environment
class GridWorld:
    def __init__(self, size, start, goal, obstacles):
        self.size = size
        self.start = start
        self.goal = goal
        self.obstacles = obstacles
        self.reset()
    
    def reset(self):
        self.agent_pos = list(self.start)
        return tuple(self.agent_pos)
    
    def step(self, action):
        if action == 'up' and self.agent_pos[1] > 0:
            self.agent_pos[1] -= 1
        elif action == 'down' and self.agent_pos[1] < self.size[1] - 1:
            self.agent_pos[1] += 1
        elif action == 'left' and self.agent_pos[0] > 0:
            self.agent_pos[0] -= 1
        elif action == 'right' and self.agent_pos[0] < self.size[0] - 1:
            self.agent_pos[0] += 1
        
        if self.agent_pos == list(self.goal):
            return tuple(self.agent_pos), 1, True  # Reached goal
        elif tuple(self.agent_pos) in self.obstacles:
            return tuple(self.agent_pos), -1, True  # Hit an obstacle
        else:
            return tuple(self.agent_pos), -0.1, False  # Normal move

# Define actions
actions = ['up', 'down', 'left', 'right']

# Q-learning Agent
class QLearningAgent:
    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0, exploration_decay=0.99):
        self.env = env
        self.q_table = {}
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.exploration_rate = exploration_rate
        self.exploration_decay = exploration_decay

    def get_q_value(self, state, action):
        return self.q_table.get((state, action), 0.0)

    def choose_action(self, state):
        if random.random() < self.exploration_rate:
            return random.choice(actions)
        q_values = [self.get_q_value(state, action) for action in actions]
        max_q = max(q_values)
        return actions[q_values.index(max_q)]

    def learn(self, state, action, reward, next_state):
        old_q = self.get_q_value(state, action)
        next_max_q = max([self.get_q_value(next_state, a) for a in actions])
        new_q = (1 - self.learning_rate) * old_q + self.learning_rate * (reward + self.discount_factor * next_max_q)
        self.q_table[(state, action)] = new_q

    def train(self, episodes):
        for episode in range(episodes):
            state = self.env.reset()
            done = False
            while not done:
                action = self.choose_action(state)
                next_state, reward, done = self.env.step(action)
                self.learn(state, action, reward, next_state)
                state = next_state
            self.exploration_rate *= self.exploration_decay

# Training the agent
env = GridWorld(size=(5, 5), start=(0, 0), goal=(4, 4), obstacles={(1, 1), (2, 2), (3, 3)})
agent = QLearningAgent(env)
agent.train(episodes=1000)

# Test the trained agent
state = env.reset()
done = False
while not done:
    action = agent.choose_action(state)
    state, reward, done = env.step(action)
    print(f"Action: {action}, State: {state}, Reward: {reward}")
